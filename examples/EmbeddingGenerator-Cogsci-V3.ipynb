{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sentences with TreeRNNs\n",
    "\n",
    "This notebook goes through a minimal example of encoding one sentence into a distributed representation using a TreeRNN, and the using this distributed representation to generate another sentence using a different TreeRNN in reverse. To start, we'll do some data cleaning to make sure we have a good set of sentence pairs to train on. The main goal here is to remove sentences with mispelled words and oddities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import enchant \n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "from pysem.corpora import SNLI\n",
    "from pysem.networks import DependencyNetwork\n",
    "from pysem.generatives import EmbeddingGenerator\n",
    "\n",
    "checker = enchant.Dict('en_US')\n",
    "TrainingPair = namedtuple('TrainingPair', ['sentence1', 'sentence2', 'label'])\n",
    "\n",
    "snli = SNLI('/Users/peterblouw/corpora/snli_1.0/')\n",
    "snli.load_xy_pairs()\n",
    "\n",
    "def repair(sen):\n",
    "    tokens = DependencyNetwork.parser(sen)\n",
    "    if len(tokens) > 15:\n",
    "        return None\n",
    "    for token in tokens:\n",
    "        if not checker.check(token.text):\n",
    "            return None\n",
    "    return sen\n",
    "\n",
    "def clean_data(data):\n",
    "    clean = []\n",
    "    for item in data:\n",
    "        \n",
    "        s1 = repair(item.sentence1)\n",
    "        s2 = repair(item.sentence2)\n",
    "        if s1 == None or s2 == None:\n",
    "            continue\n",
    "        else:\n",
    "            clean.append(TrainingPair(s1, s2, item.label))\n",
    "    \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_dev = clean_data(snli.dev_data[:100])\n",
    "clean_train = clean_data(snli.train_data[:1000])\n",
    "clean_test = clean_data(snli.test_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "4\n",
      "486\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_dev))\n",
    "print(len(clean_test))\n",
    "print(len(clean_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll build a vocab from the set of cleaned sentence pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_vocab(data):\n",
    "    vocab = set()\n",
    "    for item in data:\n",
    "        s1 = item.sentence1\n",
    "        s2 = item.sentence2\n",
    "        \n",
    "        t1 = DependencyNetwork.parser(s1)\n",
    "        t2 = DependencyNetwork.parser(s2)\n",
    "        \n",
    "        for t in t1:\n",
    "            if t.text not in vocab:\n",
    "                vocab.add(t.text)\n",
    "        for t in t2:\n",
    "            if t.text not in vocab:\n",
    "                vocab.add(t.text)\n",
    "\n",
    "    return sorted(list(vocab))\n",
    "\n",
    "data = clean_dev + clean_test + clean_train\n",
    "vocab = build_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can collect all of the sentence pairs standing in entailment relations to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n",
      "1\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "train_data = [d for d in clean_train if d.label == 'entailment'] # or d.label == 'neutral']\n",
    "test_data = [d for d in clean_test if d.label == 'entailment'] # or d.label == 'neutral']\n",
    "dev_data = [d for d in clean_dev if d.label == 'entailment'] # or d.label == 'neutral']\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(len(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On iteration  0\n"
     ]
    }
   ],
   "source": [
    "from pysem.utils.snli import InferentialRoleModel\n",
    "\n",
    "dim = 300\n",
    "iters = 1\n",
    "rate = 0.01\n",
    "\n",
    "vectors = 'w2v_embeddings.pickle'\n",
    "\n",
    "with open('w2v_dep_vocabs.pickle', 'rb') as pfile:\n",
    "    subvocabs = pickle.load(pfile)\n",
    "\n",
    "encoder = DependencyNetwork(dim=dim, vocab=vocab, pretrained=vectors)\n",
    "decoder = EmbeddingGenerator(dim=dim, subvocabs=subvocabs, vectors=vectors)\n",
    "\n",
    "model = InferentialRoleModel(encoder=encoder, decoder=decoder, data=train_data)\n",
    "model.train(iters=iters, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingPair(sentence1='A couple walk through a white brick town.', sentence2='People are walking outdoors.', label='entailment')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'people are walking outdoors .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = random.choice(train_data)\n",
    "\n",
    "print(sample)\n",
    "\n",
    "model.encode(sample.sentence1)\n",
    "model.decode(sample.sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5975206611570248\n",
      "0.29411764705882354\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(data, model):\n",
    "    total = 0 \n",
    "    correct = 0\n",
    "\n",
    "    for item in data:\n",
    "        model.encoder.forward_pass(item.sentence1)\n",
    "        model.decoder.forward_pass(item.sentence2, model.encoder.get_root_embedding())\n",
    "\n",
    "        for node in model.decoder.tree:\n",
    "            total += 1\n",
    "            if node.pword.lower() == node.lower_:\n",
    "                correct += 1\n",
    "\n",
    "    return float(correct / total)\n",
    "\n",
    "print(compute_accuracy(train_data, model))\n",
    "print(compute_accuracy(dev_data, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('enc_model.pickle','dec_model.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5975206611570248\n",
      "0.29411764705882354\n"
     ]
    }
   ],
   "source": [
    "test_model = InferentialRoleModel(encoder=None, decoder=None, data=train_data)\n",
    "test_model.load('enc_model.pickle','dec_model.pickle')\n",
    "\n",
    "print(compute_accuracy(train_data, test_model))\n",
    "print(compute_accuracy(dev_data, test_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Entailment Generation Examples\n",
    "\n",
    "This small amount of data probably isn't enough to generalize outside of the training set, so we'll first check how well the learned decoder is able to generate the entailments it has been trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
