{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Embeddings from Wikipedia Text\n",
    "We can use a stream of articles from wikipedia as a corpus for learning useful representations that capture various relationships between words. First, we'll set up the corpus, build a vocabulary of words to model, and check the size of this vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  14648\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pysem.embeddings import ContextEmbedding, OrderEmbedding, SyntaxEmbedding\n",
    "from pysem.corpora import Wikipedia\n",
    "\n",
    "wiki = Wikipedia('/Users/peterblouw/corpora/wikipedia', article_limit=100)\n",
    "wiki.build_vocab()\n",
    "\n",
    "print('Vocab size: ', len(wiki.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Encoding\n",
    "Now, we can build a basic random indexing model to encode the word co-occurence patterns in these articles into a set of high-dimensional vectors. This method is related to well-known algorithms such as LSA, Word2Vec (i.e. CBOW + skip-gram encoding), and GloVe. One benefit of random indexing is that it is easy to parallelize, and hence efficient to run on large corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim = 512\n",
    "\n",
    "model = ContextEmbedding(corpus=wiki, vocab=wiki.vocab)\n",
    "model.train(dim=dim, batchsize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the nearest neighbors to any word in the resulting 'semantic space' with just a few lines of code. Note that with this small amount of training data, the results will be specific to the topics of the wikipedia articles that have been chosen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors to \"brain\":\n",
      "brain 1.0\n",
      "autism 0.54344561018\n",
      "disorders 0.469028282092\n",
      "pathophysiology 0.466330738368\n",
      "abnormal 0.441061449132\n",
      "\n",
      "Nearest neighbors to \"movie\":\n",
      "movie 1.0\n",
      "conferred 0.519436362895\n",
      "chances 0.431285975196\n",
      "spend 0.428732468579\n",
      "awards 0.414598293813\n",
      "\n",
      "Nearest neighbors to \"king\":\n",
      "king 1.0\n",
      "tsar 0.500444391653\n",
      "elisabeth 0.495399346033\n",
      "empress 0.462959086472\n",
      "austria 0.460324661169\n",
      "\n",
      "Nearest neighbors to \"wine\":\n",
      "wine 1.0\n",
      "fabrics 0.520567542197\n",
      "shipments 0.491140089023\n",
      "grain 0.479177993768\n",
      "lamps 0.473673349113\n",
      "\n",
      "Nearest neighbors to \"football\":\n",
      "football 1.0\n",
      "team 0.733451490448\n",
      "national 0.574808780484\n",
      "league 0.552105716309\n",
      "competitions 0.53870027669\n",
      "\n",
      "Nearest neighbors to \"president\":\n",
      "president 1.0\n",
      "lincoln 0.583303474198\n",
      "government 0.572080966555\n",
      "power 0.540296170581\n",
      "elected 0.540091293746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_list = ['brain','movie','king','wine','football','president']\n",
    "\n",
    "for word in word_list:\n",
    "    print('Nearest neighbors to \"%s\":' % word)\n",
    "    model.get_nearest(word)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order Encoding\n",
    "\n",
    "To make the model a bit more interesting, we can encode positional information about the words that tend to occur around each target word in our voabulary. This amounts to adding information about ngrams in the corpus to each semantic vector. Computation is a bit more costly in this case, due to the need to compute several circular convolutions per word occurence. Again, though, this computation can be parallelized, so it's not too bad to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = OrderEmbedding(corpus=wiki, vocab=wiki.vocab)\n",
    "model.train(dim=dim, batchsize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting vectors can be queried for likely words occuring in positions to left and right of a target word. We can also find words that tend to occur in the same 'order contexts' as a target word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likely words next to \"president\":\n",
      "of 0.382755494788\n",
      "the 0.180164686655\n",
      "open 0.15547932252\n",
      "organ 0.153229070419\n",
      "rank 0.153081494071\n",
      "\n",
      "Likely words next to \"abraham\":\n",
      "lincoln 0.658149572882\n",
      "became 0.272971910354\n",
      "four 0.175399749547\n",
      "was 0.170755109111\n",
      "microvertebrate 0.16943944256\n",
      "\n",
      "Likely words next to \"of\":\n",
      "the 0.613220994693\n",
      "fame 0.171407702238\n",
      "delegation 0.169932453691\n",
      "indus 0.168967915638\n",
      "fort 0.162275386815\n",
      "\n",
      "Likely words next to \"academy\":\n",
      "awards 0.387565945673\n",
      "of 0.23705046183\n",
      "award 0.185014873278\n",
      "cultivated 0.176263114853\n",
      "renovated 0.167384534068\n",
      "\n",
      "Likely words next to \"argued\":\n",
      "that 0.839421791859\n",
      "in 0.15748030515\n",
      "inclined 0.156978640176\n",
      "confirm 0.153613452494\n",
      "attracting 0.151525390146\n",
      "\n",
      "Likely words next to \"give\":\n",
      "the 0.21482697857\n",
      "settlement 0.180179484466\n",
      "up 0.179351508232\n",
      "outsourcing 0.157924780219\n",
      "buy 0.157780325143\n",
      "\n",
      "Likely words next to \"each\":\n",
      "other 0.484684055273\n",
      "of 0.26651835649\n",
      "minute 0.170385495395\n",
      "microvertebrate 0.169999011835\n",
      "armenia 0.169330322612\n",
      "\n",
      "Likely words next to \"smallest\":\n",
      "alkali 0.197430965941\n",
      "amphibian 0.182386355759\n",
      "class 0.177238551683\n",
      "roots 0.170399212681\n",
      "tartaric 0.164607109906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_list = ['president','abraham','of','academy','argued','give','each','smallest']\n",
    "\n",
    "for word in word_list:\n",
    "    print('Likely words next to \"%s\":' % word)\n",
    "    model.get_completions(word, position=1)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a more accurate way to find preceding and subsequent words - we simply look for order vectors that tend to encode the target word in particular positions. (It's helpful to consider why this is more accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase completion for promoted __ rights\n",
      "womens 0.295232661799\n",
      "voting 0.260559512777\n",
      "human 0.174971002393\n",
      "exploitation 0.168301908727\n",
      "gross 0.162211002789\n",
      "\n",
      "Phrase completion for which lincoln promoted __ rights for\n",
      "voting 0.285143378689\n",
      "descriptive 0.272722302065\n",
      "practically 0.26657383359\n",
      "filters 0.262916394025\n",
      "taxable 0.259761047626\n",
      "\n",
      "Phrase completion for president __\n",
      "abdelaziz 0.453357412357\n",
      "sali 0.406385216355\n",
      "richard 0.395164728279\n",
      "nixons 0.367285183688\n",
      "franklin 0.363557401577\n",
      "\n",
      "Phrase completion for  __ civil war\n",
      "yearlong 0.452919201331\n",
      "spanish 0.274829115928\n",
      "devastating 0.265167431973\n",
      "commanderinchief 0.230518228242\n",
      "ensuing 0.20712482073\n",
      "\n",
      "Phrase completion for aristotle held more __ theories\n",
      "accurate 0.329408727259\n",
      "definitely 0.285656253704\n",
      "efficient 0.275589221452\n",
      "than 0.249757187713\n",
      "distant 0.231377148262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phrase_list = [ 'promoted __ rights', 'which lincoln promoted __ rights for', 'president __', \n",
    "               ' __ civil war', 'aristotle held more __ theories']\n",
    "\n",
    "for phrase in phrase_list:\n",
    "    print('Phrase completion for %s' % phrase)\n",
    "    model.get_resonants(phrase)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax Encoding\n",
    "It is possible to extend the methods used for encoding order information to encode information about the syntactic structure of the sentences a word typically occurs in. We'll use dependency structures to model this information, primarily because they are simpler than constituency structures and thus easier to encode in vectors with a limited capacity for storing structured information (all the usual facts about HRR capacity apply here). Instead of encoding words to the left or right of a target word, we'll encode words that occur as parents or children of a target word in a dependency tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://taweb.aichi-u.ac.jp/tmgross/pix/PSG-DG.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"http://taweb.aichi-u.ac.jp/tmgross/pix/PSG-DG.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting vectors can be used to query a target word for words that are commonly linked to it by a given dependency relation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SyntaxEmbedding(corpus=wiki, vocab=wiki.vocab)\n",
    "model.train(dim=dim, batchsize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common nsubj for \"emphasized\":\n",
      "historians 0.308699289221\n",
      "iatrochemistry 0.30119167908\n",
      "anthropology 0.296073104435\n",
      "lincoln 0.259356753825\n",
      "change 0.258065401129\n",
      "\n",
      "Common nsubj for \"invited\":\n",
      "leaders 0.310169961697\n",
      "legislature 0.261423996759\n",
      "massoud 0.251087800359\n",
      "government 0.230902382508\n",
      "they 0.229253309749\n",
      "\n",
      "Common nsubj for \"appeals\":\n",
      "party 0.519144348455\n",
      "defendant 0.497877799977\n",
      "who 0.440538492479\n",
      "anarchocommunist 0.1948900773\n",
      "cfb 0.163253282246\n",
      "\n",
      "Common dobj for \"emphasized\":\n",
      "doctrine 0.358332525265\n",
      "application 0.291008756448\n",
      "opposition 0.289021486144\n",
      "independence 0.266479683281\n",
      "rights 0.260209440485\n",
      "\n",
      "Common dobj for \"invited\":\n",
      "einstein 0.326522653232\n",
      "lincoln 0.296432232376\n",
      "states 0.275441344005\n",
      "departments 0.264893546054\n",
      "priestess 0.255444756649\n",
      "\n",
      "Common dobj for \"appeals\":\n",
      "it 0.407735316625\n",
      "conviction 0.38481297082\n",
      "bees 0.166581341639\n",
      "rosa 0.159904901159\n",
      "clement 0.151520139973\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_list = ['emphasized','invited','appeals']\n",
    "\n",
    "for word in word_list:\n",
    "    print('Common nsubj for \"%s\":' % word)\n",
    "    model.get_verb_neighbors(word, 'nsubj')\n",
    "    print('')\n",
    "\n",
    "for word in word_list:\n",
    "    print('Common dobj for \"%s\":' % word)\n",
    "    model.get_verb_neighbors(word, 'dobj')\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
