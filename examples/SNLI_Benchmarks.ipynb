{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Natural Language Inference with the SNLI Dataset\n",
    "\n",
    "The Stanford Natural Language Inference (SNLI) dataset is a recently released corpus of sentence pairs labelled with inferential relationships. The first sentence in each pair can either entail, contradict, or be neutral with respect to the second sentence, and our goal is build models that learn to predict this relationship for novel sentence pairs. \n",
    "The dataset consists of a training set of ~500,000 labelled sentence pairs, along with development and test sets that each contain ~10,000 labelled sentence pairs. More information about the dataset can be found here.\n",
    "\n",
    "In the rest of this notebook, we'll compare a number of methods for learning to label the sentence pairs in this dataset. As simple baseline, we'll first build distributed bag-of-words representations for each sentence, and then use these sentence representations as input to a multilayer perceptron that predicts a class label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag-of-Words with Random Embeddings\n",
    "\n",
    "First, we'll use sentence representations that are a sum of intially random word embeddings that are learned over the course of training. To start, we'll load the dataset and build a vocabulary. Then, we'll extract the labelled sentence pairs for the training and development sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pysem.corpora import SNLI\n",
    "from pysem.utils.ml import MultiLayerPerceptron\n",
    "from pysem.utils.vsa import normalize\n",
    "\n",
    "snli = SNLI(path='/Users/peterblouw/corpora/snli_1.0/')\n",
    "snli.build_vocab()\n",
    "\n",
    "snli.extractor = snli.get_xy_pairs\n",
    "train_data = [pair for pair in snli.train_data if pair.label != '-']\n",
    "dev_data = [pair for pair in snli.dev_data if pair.label != '-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use scikit-learn to make a count vectorizer that converts sentences into binary vectors. The vectors will be used to extract the correct word embeddings from an initially random embedding matrix. 300 dimensional embeddings will be used for consistency with the pretrained embeddings to be used in the next example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "dim = 300\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "vectorizer.fit(snli.vocab)\n",
    "\n",
    "embedding_matrix = np.random.normal(loc=0, scale=1/np.sqrt(dim), size=(dim, len(vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an MLP for label prediction it's now possible to learn a set of embeddings and classifier parameter on the training data and then see how well things generalize to the development data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = MultiLayerPerceptron(di=dim*2, dh=dim, do=3)\n",
    "\n",
    "minibatch_size = 100\n",
    "iters = 1000\n",
    "rate = 0.1\n",
    "\n",
    "# Train using randomly selected minibatches to get roughly 5 epochs\n",
    "for _ in range(iters):\n",
    "    minibatch = random.sample(train_data, minibatch_size)\n",
    "\n",
    "    s1s = [sample.sentence1 for sample in minibatch]\n",
    "    s2s = [sample.sentence2 for sample in minibatch]\n",
    "    \n",
    "    s1_indicators = vectorizer.transform(s1s).toarray().T\n",
    "    s2_indicators = vectorizer.transform(s2s).toarray().T\n",
    "    \n",
    "    s1_embeddings = np.dot(embedding_matrix, s1_indicators)\n",
    "    s2_embeddings = np.dot(embedding_matrix, s2_indicators)\n",
    "\n",
    "    # concatenate the sentence representations for input to classifier\n",
    "    xs = np.vstack((s1_embeddings, s2_embeddings))\n",
    "    ys = snli.binarize([sample.label for sample in minibatch])\n",
    "\n",
    "    # update the classifier parameters\n",
    "    classifier.train(xs, ys, rate=rate)\n",
    "    \n",
    "    # update the embedding matrix\n",
    "    s1s_grad = np.dot(classifier.yi_grad[:dim], s1_indicators.T) \n",
    "    s2s_grad = np.dot(classifier.yi_grad[dim:], s2_indicators.T)\n",
    "    embedding_grad = (s1s_grad + s2s_grad) / minibatch_size\n",
    "    embedding_matrix -= rate * embedding_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After training, the accuracy on the training set and the development set can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "4842\n",
      "0\n",
      "Accuracy:  0.358057305426\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def compute_accuracy(data):\n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    batchsize = 5000\n",
    "        \n",
    "    while True:\n",
    "        batch = list(islice(data, batchsize))\n",
    "        n_total += len(batch)\n",
    "        if len(batch) == 0:\n",
    "            break\n",
    "\n",
    "        s1s = [sample.sentence1 for sample in batch]\n",
    "        s2s = [sample.sentence2 for sample in batch]\n",
    "    \n",
    "        s1_indicators = vectorizer.transform(s1s).toarray().T\n",
    "        s2_indicators = vectorizer.transform(s2s).toarray().T\n",
    "\n",
    "        s1_embeddings = np.dot(embedding_matrix, s1_indicators)\n",
    "        s2_embeddings = np.dot(embedding_matrix, s2_indicators)\n",
    "    \n",
    "        xs = np.vstack((s1_embeddings, s2_embeddings))\n",
    "        ys = snli.binarize([sample.label for sample in batch])\n",
    "        \n",
    "        predictions = classifier.predict(xs)\n",
    "        n_correct += sum(np.equal(predictions, np.argmax(ys, axis=0)))\n",
    "        \n",
    "    print('Accuracy: ', n_correct / n_total)\n",
    "\n",
    "compute_accuracy((i for i in dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
