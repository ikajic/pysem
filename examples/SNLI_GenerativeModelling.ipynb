{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Entailments from Sentence Representations\n",
    "\n",
    "A natural extension of the material covered in the previous notebook involves attempting to generate further sentences that are entailed by a given sentence. This generation procedure could then be carried out repeatedly to create complex networks of entailment relations amongst sentences. Such a network might then considered as a formalization of the \"inferential roles\" of the various sentences it contains. And more interestingly, question answering can be formulated as the generation of a specific entailed sentence conditional upon a query. \n",
    "\n",
    "This is all still very much a work in progress. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings Assuming a Tree Structure\n",
    "\n",
    "To approach the problem, we'll first encode a sentence into distributed representation using a tree-structured neural network of sort discussed previously. Then, we'll use a similar network to \"decode out\" from this representation an entailed sentence in a step-by-step manner. To simplify matters, we'll assume that the structure of the decoded sentences is given (i.e. the the structure of the parse tree is available), and aim to learn a set of weights for each dependency in the structure such that propagating activations through the structure results in each node being assigned an embedding that predicts the word appropriate to that node. Word prediction occurs by applying a softmax over the inner products between the embedding and vocabulary representations for each possible word; to simplify this process, we'll use subvocabularies that include only the words that could occupy a particular node given the dependency it occupies with respect to a head word. For instance, if a node occupies the 'det' dependency with respect to a head word (e.g. a noun like \"guitar\"), then only words like \"a\", \"the\", \"some\", etc. will be considered when computing the softmax. \n",
    "\n",
    "To start, we'll load the SNLI corpus and preprocess it to only consider sentence pairs that are labelled with the entailment relation. We'll use a sampling of the pairs to train our generative model, and then test how well the model generates entailments for novel sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from pysem.corpora import SNLI\n",
    "from pysem.networks import DependencyNetwork\n",
    "from pysem.generatives import EmbeddingGenerator\n",
    "\n",
    "snli = SNLI('/home/pblouw/corpora/snli_1.0/')\n",
    "snli.extractor = snli.get_xy_pairs\n",
    "snli.load_vocab('snli_vocab.pickle')\n",
    "\n",
    "with open('subvocabs.pickle', 'rb') as pfile:\n",
    "    subvocabs = pickle.load(pfile)\n",
    "\n",
    "train_data = [d for d in snli.train_data if d.label == 'entailment'] # focus on entailment relations only\n",
    "\n",
    "train_batch = train_data[:100000] # use small amount of data for prototyping\n",
    "test_batch = train_data[100000:100100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the encoder and decoder networks, and then train both using the selected training examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On iteration  0\n",
      "On iteration  1\n",
      "On iteration  2\n",
      "On iteration  3\n",
      "On iteration  4\n",
      "On iteration  5\n",
      "On iteration  6\n",
      "On iteration  7\n",
      "On iteration  8\n",
      "On iteration  9\n",
      "On iteration  10\n",
      "On iteration  11\n",
      "On iteration  12\n",
      "On iteration  13\n",
      "On iteration  14\n",
      "On iteration  15\n",
      "On iteration  16\n",
      "On iteration  17\n",
      "On iteration  18\n",
      "On iteration  19\n",
      "On iteration  20\n",
      "On iteration  21\n",
      "On iteration  22\n",
      "On iteration  23\n",
      "On iteration  24\n",
      "On iteration  25\n",
      "On iteration  26\n",
      "On iteration  27\n",
      "On iteration  28\n",
      "On iteration  29\n",
      "On iteration  30\n",
      "On iteration  31\n",
      "On iteration  32\n",
      "On iteration  33\n",
      "On iteration  34\n",
      "On iteration  35\n",
      "On iteration  36\n",
      "On iteration  37\n",
      "On iteration  38\n",
      "On iteration  39\n",
      "On iteration  40\n",
      "On iteration  41\n",
      "On iteration  42\n",
      "On iteration  43\n",
      "On iteration  44\n",
      "On iteration  45\n",
      "On iteration  46\n",
      "On iteration  47\n",
      "On iteration  48\n",
      "On iteration  49\n"
     ]
    }
   ],
   "source": [
    "dim = 200\n",
    "iters = 50\n",
    "rate = 0.002\n",
    "\n",
    "encoder = DependencyNetwork(dim=dim, vocab=snli.vocab)\n",
    "decoder = EmbeddingGenerator(dim=dim, subvocabs=subvocabs)\n",
    "\n",
    "for _ in range(iters):\n",
    "    print('On iteration ', _)\n",
    "    for sample in train_batch:\n",
    "        s1 = sample.sentence1\n",
    "        s2 = sample.sentence2\n",
    "\n",
    "        encoder.forward_pass(s1)\n",
    "        decoder.forward_pass(s2, encoder.get_root_embedding())\n",
    "        decoder.backward_pass(rate=rate)\n",
    "        encoder.backward_pass(decoder.pass_grad, rate=rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is possible to see how well the model is able to generate entailments. We can compute raw accuracies, which indicate how many entailed sentences in the training and test sets the model is able to generate completely correctly. We can also compute the proportation of correctly labelled nodes for entailments that were not generated completely correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Raw Accuracy: ', 0.04528)\n",
      "('Raw Accuracy: ', 0.0)\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(batch, encoder, decoder):\n",
    "    errors = 0\n",
    "    stats = []\n",
    "    for sample in batch:\n",
    "        s1 = sample.sentence1\n",
    "        s2 = sample.sentence2\n",
    "\n",
    "        encoder.forward_pass(s1)\n",
    "        decoder.forward_pass(s2, encoder.get_root_embedding())\n",
    "\n",
    "        for node in decoder.tree:\n",
    "            if node.lower_ != node.pword:\n",
    "                errors += 1\n",
    "                break\n",
    "\n",
    "    return 'Raw Accuracy: ', (len(batch) - errors) / len(batch)\n",
    "\n",
    "print(get_accuracy(train_batch, encoder, decoder))\n",
    "print(get_accuracy(test_batch, encoder, decoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Men and women running in a competition.\n",
      "Predicted Entailment:  are in a people running\n",
      "Actual Entailment:  humans of both sexes running\n",
      "\n",
      "Sentence:  An Asian woman wearing a Asian dress sitting among a group of cloths, with a woven basket on her lap.\n",
      "Predicted Entailment:  an asian woman is in her\n",
      "Actual Entailment:  an oriental woman sits by fabric\n",
      "\n",
      "Sentence:  Subjects eating a McDonalds meal along the street.\n",
      "Predicted Entailment:  eating eating meal\n",
      "Actual Entailment:  people eating mcdonalds\n",
      "\n",
      "Sentence:  A person stands next to the Easter Island statues.\n",
      "Predicted Entailment:  a person is on the night .\n",
      "Actual Entailment:  a person is at an island .\n",
      "\n",
      "Sentence:  A man with long hair taking pictures with his camera.\n",
      "Predicted Entailment:  man with a beard is man a facial food .\n",
      "Actual Entailment:  someone in the picture is holding an electronic device .\n",
      "\n",
      "Sentence:  A man in a jacket and tie smiles at a woman in a white dress.\n",
      "Predicted Entailment:  man are smiling next and one is smiling .\n",
      "Actual Entailment:  people are dressed nice and one is smiling .\n",
      "\n",
      "Sentence:  Three colorful clowns with ballons posing next to another plastic clown.\n",
      "Predicted Entailment:  the people are happy\n",
      "Actual Entailment:  the clowns are colorful\n",
      "\n",
      "Sentence:  The girl in the jacket is on the cement with a sucker in her mouth and a bike to the right.\n",
      "Predicted Entailment:  a girl girl clothes on her head .\n",
      "Actual Entailment:  the girl has something in her mouth .\n",
      "\n",
      "Sentence:  A little boy slides down a bright red corkscrew slide.\n",
      "Predicted Entailment:  a little boy slides on a slide .\n",
      "Actual Entailment:  a little boy plays on a slide .\n",
      "\n",
      "Sentence:  A group of skaters are at a skate park.\n",
      "Predicted Entailment:  there are a outdoors watching .\n",
      "Actual Entailment:  there are some people skating .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in random.sample(train_batch, 10):\n",
    "    s1 = sample.sentence1\n",
    "    s2 = sample.sentence2\n",
    "\n",
    "    encoder.forward_pass(s1)\n",
    "    decoder.forward_pass(s2, encoder.get_root_embedding())\n",
    "\n",
    "    predicted = [node.pword for node in decoder.tree]\n",
    "    true = [node.lower_ for node in decoder.tree]\n",
    "    \n",
    "    print('Sentence: ', s1)\n",
    "    print('Predicted Entailment: ', ' '.join(predicted))\n",
    "    print('Actual Entailment: ', ' '.join(true))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  A young woman walking on the sidewalk.\n",
      "Predicted Entailment:  a woman walking outside\n",
      "Actual Entailment:  a woman is outside\n",
      "\n",
      "Sentence:  a girl wearing pair of red leggings and a wool dress walks by the window display of a store wearing her headphones looking carefree.\n",
      "Predicted Entailment:  a woman is looks a something of medical bread .\n",
      "Actual Entailment:  a girl is wearing a pair of red leggings .\n",
      "\n",
      "Sentence:  a guy on a roof doing repairs.\n",
      "Predicted Entailment:  a man is on roof of his building outside working it .\n",
      "Actual Entailment:  a guy is on top of his house outside doing repairs .\n",
      "\n",
      "Sentence:  A black and white dog is swimming in a lake.\n",
      "Predicted Entailment:  two dog swimming outside .\n",
      "Actual Entailment:  two dogs are outside .\n",
      "\n",
      "Sentence:  A lady with a pink bike is smiling for the camera.\n",
      "Predicted Entailment:  a lady is is\n",
      "Actual Entailment:  the woman is smiling\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in random.sample(test_batch, 5):\n",
    "    s1 = sample.sentence1\n",
    "    s2 = sample.sentence2\n",
    "\n",
    "    encoder.forward_pass(s1)\n",
    "    decoder.forward_pass(s2, encoder.get_root_embedding())\n",
    "\n",
    "    predicted = [node.pword for node in decoder.tree]\n",
    "    true = [node.lower_ for node in decoder.tree]\n",
    "    \n",
    "    print('Sentence: ', s1)\n",
    "    print('Predicted Entailment: ', ' '.join(predicted))\n",
    "    print('Actual Entailment: ', ' '.join(true))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw accuracy rates are somewhat misleading, since the generation process is only counted as accurate if every node in the tree is generated correctly. So, a tree could have 90% of its nodes generated correctly, yet still be counted as an error. \n",
    "\n",
    "Otherwise, this limited illustration suggests that the model is pretty good at learning to generate the items in the training data, but that it doesn't generalize very well. This isn't surprising given the limited amount of training data. The model is also very slow to train on the full dataset, so some optimizations and improvements will likely need to be considered to scale up to more effective training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Structure and Embeddings Jointly\n",
    "\n",
    "A drawback of predicting the words that occupy each node in a tree is that the structure of the tree needs to be known ahead of time. To avoid this drawback, we'll again use a dependency network to encode the sentence, but we'll use a modified recurrent network to decode out the entailed sentence in a manner such that each step in the decoding process can be interpreted as adding a node and an edge to the tree.\n",
    "\n",
    "At the start of decoding, the representation produced by the encoding network is mapped to the decoding network's hidden state, which is then used to predict a head word, a dependency, and a dependent word (i.e. the information needed to extend the tree by one node). Embeddings corresponding to these predicted items are then provided as the input to the decoding network at the next time step. Additionally, the weights between the input layer and the hidden layer at this time step are determined by the depedency predicted in the previous time step. The new hidden state is thus determined by these weights, the input embeddings, and the hidden state at previous time step. The new hidden state is also used to predict another extension to the tree as before. Generation ceases when no new extensions to the tree are predicted at a given time step. \n",
    "\n",
    "We'll train the generative decoder using a selection of entailment pairs, and then we'll see if it is able to predict each entailed sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On iteration  0\n",
      "On iteration  1\n",
      "On iteration  2\n",
      "On iteration  3\n",
      "On iteration  4\n",
      "On iteration  5\n",
      "On iteration  6\n",
      "On iteration  7\n",
      "On iteration  8\n",
      "On iteration  9\n",
      "On iteration  10\n",
      "On iteration  11\n",
      "On iteration  12\n",
      "On iteration  13\n",
      "On iteration  14\n",
      "On iteration  15\n",
      "On iteration  16\n",
      "On iteration  17\n",
      "On iteration  18\n",
      "On iteration  19\n",
      "On iteration  20\n",
      "On iteration  21\n",
      "On iteration  22\n",
      "On iteration  23\n",
      "On iteration  24\n",
      "On iteration  25\n",
      "On iteration  26\n",
      "On iteration  27\n",
      "On iteration  28\n",
      "On iteration  29\n",
      "On iteration  30\n",
      "On iteration  31\n",
      "On iteration  32\n",
      "On iteration  33\n",
      "On iteration  34\n",
      "On iteration  35\n",
      "On iteration  36\n",
      "On iteration  37\n",
      "On iteration  38\n",
      "On iteration  39\n"
     ]
    }
   ],
   "source": [
    "from pysem.generatives import TreeGenerator\n",
    "\n",
    "encoder = DependencyNetwork(dim=dim, vocab=snli.vocab)\n",
    "decoder = TreeGenerator(dim=dim, vocab=snli.vocab)\n",
    "\n",
    "for _ in range(iters):\n",
    "    print('On iteration ', _)\n",
    "    for sample in train_batch[:10]:\n",
    "        s1 = sample.sentence1\n",
    "        s2 = sample.sentence2\n",
    "\n",
    "        encoder.forward_pass(s1)\n",
    "        decoder.forward_pass(encoder.get_root_embedding(), s2)\n",
    "        decoder.backward_pass(rate=rate)\n",
    "        encoder.backward_pass(decoder.pass_grad, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source Sentence:  Two blond women are hugging one another.\n",
      "Correct Entailment:  There are women showing affection.\n",
      "\n",
      "Predicted Head:  are    Correct Head:  are\n",
      "Predicted Dep:  ROOT    Correct Dep:  ROOT\n",
      "Predicted Token:  are    Correct Token:  are\n",
      "\n",
      "Predicted Head:  are    Correct Head:  are\n",
      "Predicted Dep:  expl    Correct Dep:  expl\n",
      "Predicted Token:  there    Correct Token:  there\n",
      "\n",
      "Predicted Head:  are    Correct Head:  are\n",
      "Predicted Dep:  attr    Correct Dep:  attr\n",
      "Predicted Token:  women    Correct Token:  women\n",
      "\n",
      "Predicted Head:  women    Correct Head:  women\n",
      "Predicted Dep:  acl    Correct Dep:  acl\n",
      "Predicted Token:  showing    Correct Token:  showing\n",
      "\n",
      "Predicted Head:  showing    Correct Head:  showing\n",
      "Predicted Dep:  dobj    Correct Dep:  dobj\n",
      "Predicted Token:  affection    Correct Token:  affection\n",
      "\n",
      "Predicted Head:  are    Correct Head:  are\n",
      "Predicted Dep:  punct    Correct Dep:  punct\n",
      "Predicted Token:  .    Correct Token:  .\n",
      "\n",
      "\n",
      "\n",
      "Source Sentence:  A Little League team tries to catch a runner sliding into a base in an afternoon game.\n",
      "Correct Entailment:  A team is trying to tag a runner out.\n",
      "\n",
      "Predicted Head:  trying    Correct Head:  trying\n",
      "Predicted Dep:  ROOT    Correct Dep:  ROOT\n",
      "Predicted Token:  trying    Correct Token:  trying\n",
      "\n",
      "Predicted Head:  trying    Correct Head:  trying\n",
      "Predicted Dep:  nsubj    Correct Dep:  nsubj\n",
      "Predicted Token:  team    Correct Token:  team\n",
      "\n",
      "Predicted Head:  trying    Correct Head:  trying\n",
      "Predicted Dep:  aux    Correct Dep:  aux\n",
      "Predicted Token:  is    Correct Token:  is\n",
      "\n",
      "Predicted Head:  trying    Correct Head:  trying\n",
      "Predicted Dep:  xcomp    Correct Dep:  xcomp\n",
      "Predicted Token:  tag    Correct Token:  tag\n",
      "\n",
      "Predicted Head:  tag    Correct Head:  tag\n",
      "Predicted Dep:  dobj    Correct Dep:  dobj\n",
      "Predicted Token:  runner    Correct Token:  runner\n",
      "\n",
      "Predicted Head:  tag    Correct Head:  tag\n",
      "Predicted Dep:  prt    Correct Dep:  prt\n",
      "Predicted Token:  out    Correct Token:  out\n",
      "\n",
      "Predicted Head:  trying    Correct Head:  trying\n",
      "Predicted Dep:  punct    Correct Dep:  punct\n",
      "Predicted Token:  .    Correct Token:  .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in random.sample(train_batch[:10], 2):\n",
    "    s1 = sample.sentence1\n",
    "    s2 = sample.sentence2\n",
    "\n",
    "    encoder.forward_pass(s1)\n",
    "    decoder.forward_pass(encoder.get_root_embedding(), s2)\n",
    "    \n",
    "    print('')\n",
    "    print('Source Sentence: ', s1)\n",
    "    print('Correct Entailment: ', s2)\n",
    "    print('')\n",
    "    for node in decoder.sequence:\n",
    "        print('Predicted Head: ', node.ph, '   Correct Head: ', node.head.lower_)\n",
    "        print('Predicted Dep: ', node.pd, '   Correct Dep: ', node.dep_)\n",
    "        print('Predicted Token: ', node.pw, '   Correct Token: ', node.lower_)\n",
    "        print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is again a very limited demonstration, but it indicates that the model on track to learn how to generate sentences by predicting structure and content jointly. One needed addition to the model is a proper prediction heuristic that samples from the distribution for each predicted word, head, and dependency to create the input to the hidden state at the next time step. Scaling and speed are also issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
